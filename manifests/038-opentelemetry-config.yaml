# file: /mnt/urbalurbadisk/manifests/038-opentelemetry-config.yaml
#
# Description:
# Helm values file for deploying OpenTelemetry Collector with the official Helm chart.
# - Configures OTLP HTTP/gRPC receivers for telemetry ingestion from applications.
# - Routes traces to Tempo, logs to Loki, and metrics to Prometheus.
# - Includes debug exporters for local development and troubleshooting.
# - Uses deployment mode for centralized telemetry collection.
# - Optimized for resource-constrained environments.
# - Ingress is disabled here (handled separately in 038-opentelemetry-ingress.yaml).
#
# Usage:
#   helm upgrade --install otel-collector open-telemetry/opentelemetry-collector \
#     --namespace monitoring --create-namespace \
#     --values manifests/038-opentelemetry-config.yaml
#
# Prerequisites:
# - Loki should be deployed in the 'monitoring' namespace on port 3100
# - Tempo should be deployed in the 'monitoring' namespace on port 4317
# - Prometheus should be deployed in the 'monitoring' namespace on port 9090
# - Traefik ingress controller should be available for separate ingress configuration

nameOverride: ""
namespaceOverride: "monitoring"

# Deployment mode - single instance for centralized collection
mode: deployment
replicaCount: 1

# Image configuration - required by the chart
image:
  repository: "ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-k8s"
  tag: ""  # Uses chart default if empty
  pullPolicy: IfNotPresent

# Command configuration for k8s distribution
command:
  name: "otelcol-k8s"

# OpenTelemetry Collector pipeline configuration
config:
  # Receivers - ingress points for telemetry data
  receivers:
    otlp:
      protocols:
        # HTTP receiver for REST-based telemetry (port 4318)
        http:
          endpoint: 0.0.0.0:4318
          cors:
            allowed_origins:
              - "*"
        # gRPC receiver for high-performance telemetry (port 4317)
        grpc:
          endpoint: 0.0.0.0:4317

  # Processors - minimal processing for resource-constrained systems
  processors:
    # Batch processor optimized for low resource usage
    batch:
      timeout: 5s
      send_batch_size: 256
    # Memory limiter with lower threshold
    memory_limiter:
      limit_mib: 100
      spike_limit_mib: 20

  # Exporters - destinations for processed telemetry data
  exporters:
    # Debug exporter with minimal output for resource constraints
    debug:
      verbosity: basic
      sampling_initial: 1
      sampling_thereafter: 1000
    
    # OTLP HTTP exporter for Loki (using native OTLP endpoint via gateway)
    otlphttp/loki:
      endpoint: http://loki-gateway.monitoring.svc.cluster.local/otlp/v1/logs
      tls:
        insecure: true
    
    # OTLP gRPC exporter for Tempo (using OTLP gRPC endpoint)
    otlp/tempo:
      endpoint: http://tempo.monitoring.svc.cluster.local:4317
      tls:
        insecure: true
    
    # OTLP HTTP exporter for metrics to Prometheus (using OTLP format)
    otlphttp/prometheus:
      endpoint: http://prometheus-server.monitoring.svc.cluster.local:9090/api/v1/otlp/v1/metrics
      tls:
        insecure: true

  # Service pipelines - define data flow from receivers to exporters
  service:
    pipelines:
      # Distributed tracing pipeline
      traces:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [otlp/tempo, debug]
      
      # Application logging pipeline
      logs:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [otlphttp/loki, debug]
      
      # Metrics collection pipeline
      metrics:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [otlphttp/prometheus, debug]

# Resource management for resource-constrained environments
resources:
  requests:
    memory: 64Mi
    cpu: 50m
  limits:
    memory: 128Mi
    cpu: 200m

# Environment variables for OpenTelemetry Collector (minimal logging)
extraEnvs:
  # Minimal logging configuration
  - name: OTEL_LOG_LEVEL
    value: "warn"
  
  # Minimal resource attributes
  - name: OTEL_RESOURCE_ATTRIBUTES
    value: "service.name=otel-collector"

# Security context for enhanced security
securityContext:
  runAsNonRoot: true
  runAsUser: 10001
  fsGroup: 10001

# Service configuration - ClusterIP since we're using separate ingress
service:
  type: ClusterIP

# No ingress configuration here - using separate ingress file (039-otel-collector-ingress.yaml)
ingress:
  enabled: false

# Autoscaling disabled for local development
autoscaling:
  enabled: false