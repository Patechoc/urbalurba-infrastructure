# 205-ollama-config.yaml
# Description:
# Deploys Ollama as a standalone service using the official Ollama Helm chart
# Configured for Apple M2 CPU (ARM64) in Rancher Desktop
# Configured to use the Qwen3-0.6B model (600M parameters)
# Includes persistent storage for models
# Uses the 'ai' namespace for all resources
# OPTIMIZED: Reduced memory requirements from 4Gi/8Gi to 1.5Gi/2Gi for Qwen3:0.6b model
#
# Usage:
# Add the Ollama Helm repository:
# helm repo add ollama-helm https://otwld.github.io/ollama-helm/
# helm repo update
#
# Usage:
# installing: helm install ollama ollama-helm/ollama -f 205-ollama-config.yaml -n ai --create-namespace
# upgrading:  helm upgrade ollama ollama-helm/ollama -f 205-ollama-config.yaml -n ai
# uninstalling: helm uninstall ollama --namespace ai
#
# Debug commands:
# kubectl logs -f deployment/ollama -n ai | grep -v 'GET      "/"'
# kubectl exec -it deployment/ollama -n ai -- ollama list
# kubectl exec -it deployment/ollama -n ai -- ollama pull Qwen3-0.6B
# kubectl exec -it deployment/ollama -n ai -- ollama rm model-name
# kubectl exec -it deployment/ollama -n ai -- ollama show Qwen3-0.6B

# Override the name to "ollama"
fullnameOverride: "ollama"

# Specify the namespace
# Note: This won't create the namespace automatically
# Use the 'ai' namespace for all Ollama-related resources
namespaceOverride: "ai"

# Number of replicas - keeping at 1 for model consistency
replicaCount: 1

# Container resource limits optimized for Qwen3:0.6b model (523MB)
# OPTIMIZED: Reduced from 4Gi/8Gi to 1.5Gi/2Gi based on actual usage
resources:
  requests:
    memory: 1.5Gi  # Reduced from 4Gi - matches actual usage ~1.86Gi
    cpu: 500m      # Reduced from 1500m - sufficient for small model
  limits:
    memory: 2Gi    # Reduced from 8Gi - provides headroom for spikes
    cpu: 1         # Reduced from 2000m - adequate for single model

# Persistent storage for model files
persistentVolume:
  enabled: true
  existingClaim: "ollama-models"
  accessModes:
    - ReadWriteOnce
  size: 15Gi  # Size still defined but won't be used when existingClaim is set
  
# Ollama specific configuration
ollama:
  # Configure model pulling and loading at startup
  models:
    pull:
      - qwen3:0.6b
    run:
      - qwen3:0.6b
  
  # Model configuration optimized for M2 CPU and small model
  modelConfig:
    context_length: 32768  # Qwen supports 32K context
    num_threads: 8       # M2 has 8 cores
    batch_size: 256      # Conservative for CPU processing
    temperature: 0.7     # Balanced creativity and consistency
    top_p: 0.9          # Nucleus sampling for better quality
    top_k: 40           # Limit vocabulary for faster responses
  
  # GPU acceleration disabled for M2
  # Reason: M2's integrated GPU is not accessible in Rancher Desktop's Kubernetes environment.
  # The container runs in a VM that doesn't expose GPU capabilities, and no GPU device plugins
  # are configured in the cluster. Running on CPU is optimal for this setup.
  gpu:
    enabled: false

# Service configuration to expose Ollama
service:
  type: ClusterIP
  port: 11434
  
# Adjust the probes for better stability on CPU with small model
livenessProbe:
  enabled: true
  initialDelaySeconds: 90
  periodSeconds: 15
  timeoutSeconds: 10
  failureThreshold: 6

readinessProbe:
  enabled: true
  initialDelaySeconds: 45
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6

# Optional environment variables optimized for M2 CPU and small model
extraEnv:
  - name: OLLAMA_HOST
    value: "0.0.0.0"
  - name: OLLAMA_COMPUTE_BACKEND
    value: "cpu"
  - name: OLLAMA_CPU_THREADS
    value: "8"
  - name: OLLAMA_BATCH_SIZE
    value: "256"
  - name: OLLAMA_MODEL_PARAMS
    value: '{"enable_thinking": true}'  # Enable hybrid thinking mode
  - name: OLLAMA_ORIGINS
    value: "*"  # Allow all origins for API access
  - name: OLLAMA_KEEP_ALIVE
    value: "5m"  # Keep model loaded in memory
  # Memory optimization for small model
  - name: OLLAMA_MAX_LOADED_MODELS
    value: "1"   # Limit to one model for resource efficiency
  - name: OLLAMA_NUM_PARALLEL
    value: "2"   # Reduced parallel requests for small model

# Deployment update strategy
# Using Recreate since model loading doesn't support multiple concurrent instances well
updateStrategy:
  type: "Recreate"