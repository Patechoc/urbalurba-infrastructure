
# 331-sample-data-sparkapplication.yaml
#
# Description:
# Sample SparkApplication for testing the Apache Spark Kubernetes Operator
# Runs a Spark Pi calculation to demonstrate distributed computing capabilities
#
# Part of: Databricks Replacement Project - Phase 1 (Processing Engine)
# Usage: kubectl apply -f manifests/331-sample-data-sparkapplication.yaml
#
# Monitor: kubectl get sparkapp -n spark-operator -w
# Logs: kubectl logs -n spark-operator -l app.kubernetes.io/name=spark-pi-sample
# Clean up: kubectl delete sparkapp spark-pi-sample -n spark-operator

apiVersion: spark.apache.org/v1alpha1
kind: SparkApplication
metadata:
  name: spark-pi-sample
  namespace: spark-operator
  labels:
    app: databricks-replacement
    component: spark-test
    version: "4.0.0"
spec:
  # Application details
  mainClass: "org.apache.spark.examples.SparkPi"
  jars: "local:///opt/spark/examples/jars/spark-examples.jar"
  
  # Spark configuration
  sparkConf:
    spark.dynamicAllocation.enabled: "false"
    spark.kubernetes.authenticate.driver.serviceAccountName: "spark"
    spark.kubernetes.container.image: "apache/spark:4.0.0"
    spark.driver.cores: "1"
    spark.driver.memory: "512m"
    spark.executor.instances: "2"
    spark.executor.cores: "1"
    spark.executor.memory: "512m"
    spark.sql.adaptive.enabled: "true"
    spark.sql.adaptive.coalescePartitions.enabled: "true"
  
  # Runtime versions
  runtimeVersions:
    scalaVersion: "2.13"
    sparkVersion: "4.0.0"
  
  # Resource management
  applicationTolerations:
    resourceRetainPolicy: OnFailure
    
  # Driver configuration
  driver:
    cores: 1
    memory: "512m"
    serviceAccount: spark
    labels:
      version: "4.0.0"
      component: spark-driver
    annotations:
      example.com/managed-by: "databricks-replacement"
  
  # Executor configuration  
  executor:
    cores: 1
    instances: 2
    memory: "512m"
    labels:
      version: "4.0.0"
      component: spark-executor
    annotations:
      example.com/managed-by: "databricks-replacement"